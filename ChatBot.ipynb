{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ChatBot.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTOVAbnvnNMf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "eb79d993-50e3-47e0-aca9-b7f68e597629"
      },
      "source": [
        "import re\n",
        "from time import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, RepeatVector, concatenate, TimeDistributed\n",
        "from keras.models import Model\n",
        "from keras.models import load_model\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import np_utils\n",
        "from nltk.tokenize import casual_tokenize\n",
        "from sklearn.externals import joblib\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mz5RcaqNnVRd"
      },
      "source": [
        "class chatbot:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.max_vocab_size = 50000\n",
        "        self.max_seq_len = 30\n",
        "        self.embedding_dim = 100\n",
        "        self.hidden_state_dim = 100\n",
        "        self.epochs = 500\n",
        "        self.batch_size = 128\n",
        "        self.learning_rate = 1e-4\n",
        "        self.dropout = 0.3\n",
        "        self.data_path = \"twcs.csv\"\n",
        "        self.outpath = \"\"\n",
        "        self.version = 'v1'\n",
        "        self.mode = 'train'\n",
        "        self.num_train_records = 50000\n",
        "        self.load_model_from = \"s2s_model_v1_.h5\"\n",
        "        self.vocabulary_path = \"vocabulary.pkl\"\n",
        "        self.reverse_vocabulary_path = \"reverse_vocabulary.pkl\"\n",
        "        self.count_vectorizer_path = \"count_vectorizer.pkl\"\n",
        "\n",
        "        self.UNK = 0\n",
        "        self.PAD = 1\n",
        "        self.START = 2\n",
        "\n",
        "    def process_data(self, path):\n",
        "        data = pd.read_csv(path)\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            data = pd.read_csv(path)\n",
        "            data['in_response_to_tweet_id'].fillna(-12345, inplace=True)\n",
        "            tweets_in = data[data['in_response_to_tweet_id'] == -12345]\n",
        "            tweets_in_out = tweets_in.merge(data, left_on=['tweet_id'], right_on=['in_response_to_tweet_id'])\n",
        "            return tweets_in_out[:self.num_train_records]\n",
        "        elif self.mode == 'inference':\n",
        "            return data\n",
        "\n",
        "    def replace_anonymized_names(self, data):\n",
        "\n",
        "        def replace_name(match):\n",
        "            cname = match.group(2).lower()\n",
        "            if not cname.isnumeric():\n",
        "                return match.group(1) + match.group(2)\n",
        "            return '@__cname__'\n",
        "\n",
        "        re_pattern = re.compile('(@|^@)([a-zA-Z0-9_]+)')\n",
        "        if self.mode == 'train':\n",
        "\n",
        "            in_text = data['text_x'].apply(lambda txt: re_pattern.sub(replace_name, txt))\n",
        "            out_text = data['text_y'].apply(lambda txt: re_pattern.sub(replace_name, txt))\n",
        "            return list(in_text.values), list(out_text.values)\n",
        "        else:\n",
        "            return list(map(lambda x: re_pattern.sub(replace_name, x), data))\n",
        "\n",
        "    def tokenize_text(self, in_text, out_text):\n",
        "        count_vectorizer = CountVectorizer(tokenizer=casual_tokenize, max_features=self.max_vocab_size - 3)\n",
        "        count_vectorizer.fit(in_text + out_text)\n",
        "        self.analyzer = count_vectorizer.build_analyzer()\n",
        "        self.vocabulary = {key_: value_ + 3 for key_, value_ in count_vectorizer.vocabulary_.items()}\n",
        "        self.vocabulary['UNK'] = self.UNK\n",
        "        self.vocabulary['PAD'] = self.PAD\n",
        "        self.vocabulary['START'] = self.START\n",
        "        self.reverse_vocabulary = {value_: key_ for key_, value_ in self.vocabulary.items()}\n",
        "        joblib.dump(self.vocabulary, self.outpath + 'vocabulary.pkl')\n",
        "        joblib.dump(self.reverse_vocabulary, self.outpath + 'reverse_vocabulary.pkl')\n",
        "        joblib.dump(count_vectorizer, self.outpath + 'count_vectorizer.pkl')\n",
        "        \n",
        "\n",
        "    def words_to_indices(self, sent):\n",
        "        word_indices = [self.vocabulary.get(token, self.UNK) for token in self.analyzer(sent)] + [\n",
        "            self.PAD] * self.max_seq_len\n",
        "        word_indices = word_indices[:self.max_seq_len]\n",
        "        return word_indices\n",
        "\n",
        "    def indices_to_words(self, indices):\n",
        "        return ' '.join(self.reverse_vocabulary[id] for id in indices if id != self.PAD).strip()\n",
        "\n",
        "    def data_transform(self, in_text, out_text):\n",
        "        X = [self.words_to_indices(s) for s in in_text]\n",
        "        Y = [self.words_to_indices(s) for s in out_text]\n",
        "        return np.array(X), np.array(Y)\n",
        "\n",
        "    def train_test_split_(self, X, Y):\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=0)\n",
        "        y_train = y_train[:, :, np.newaxis]\n",
        "        y_test = y_test[:, :, np.newaxis]\n",
        "        print(\"traintestsplit \",X_train.shape,X_test.shape,y_train.shape,y_test.shape)\n",
        "        return X_train, X_test, y_train, y_test\n",
        "\n",
        "    def data_creation(self):\n",
        "        data = self.process_data(self.data_path)\n",
        "        in_text, out_text = self.replace_anonymized_names(data)\n",
        "        test_sentences = []\n",
        "        test_indexes = np.random.randint(1, self.num_train_records, 10)\n",
        "        for ind in test_indexes:\n",
        "            sent = in_text[ind]\n",
        "            test_sentences.append(sent)\n",
        "        self.tokenize_text(in_text, out_text)\n",
        "        X, Y = self.data_transform(in_text, out_text)\n",
        "        X_train, X_test, y_train, y_test = self.train_test_split_(X, Y)\n",
        "        return X_train, X_test, y_train, y_test, test_sentences\n",
        "\n",
        "    def define_model(self):\n",
        "\n",
        "        # Embedding Layer\n",
        "        embedding = Embedding(\n",
        "            output_dim=self.embedding_dim,\n",
        "            input_dim=self.max_vocab_size,\n",
        "            input_length=self.max_seq_len,\n",
        "            name='embedding',\n",
        "        )\n",
        "        # Encoder input\n",
        "        encoder_input = Input(\n",
        "            shape=(self.max_seq_len,),\n",
        "            dtype='int32',\n",
        "            name='encoder_input',\n",
        "        )\n",
        "        embedded_input = embedding(encoder_input)\n",
        "\n",
        "        encoder_rnn = LSTM(\n",
        "            self.hidden_state_dim,\n",
        "            name='encoder',\n",
        "            dropout=self.dropout\n",
        "        )\n",
        "        # Context is repeated to the max sequence length so that the same context\n",
        "        # can be feed at each step of decoder\n",
        "        context = RepeatVector(self.max_seq_len)(encoder_rnn(embedded_input))\n",
        "\n",
        "        # Decoder\n",
        "        last_word_input = Input(\n",
        "            shape=(self.max_seq_len,),\n",
        "            dtype='int32',\n",
        "            name='last_word_input',\n",
        "        )\n",
        "        embedded_last_word = embedding(last_word_input)\n",
        "        # Combines the context produced by the encoder and the last word uttered as inputs\n",
        "        # to the decoder.\n",
        "\n",
        "        decoder_input = concatenate([embedded_last_word, context], axis=2)\n",
        "\n",
        "        # return_sequences causes LSTM to produce one output per timestep instead of one at the\n",
        "        # end of the intput, which is important for sequence producing models.\n",
        "        decoder_rnn = LSTM(\n",
        "            self.hidden_state_dim,\n",
        "            name='decoder',\n",
        "            return_sequences=True,\n",
        "            dropout=self.dropout\n",
        "        )\n",
        "\n",
        "        decoder_output = decoder_rnn(decoder_input)\n",
        "\n",
        "        # TimeDistributed allows the dense layer to be applied to each decoder output per timestep\n",
        "        next_word_dense = TimeDistributed(\n",
        "            Dense(int(self.max_vocab_size / 20), activation='relu'),\n",
        "            name='next_word_dense',\n",
        "        )(decoder_output)\n",
        "\n",
        "        next_word = TimeDistributed(\n",
        "            Dense(self.max_vocab_size, activation='softmax'),\n",
        "            name='next_word_softmax'\n",
        "        )(next_word_dense)\n",
        "\n",
        "        return Model(inputs=[encoder_input, last_word_input], outputs=[next_word])\n",
        "\n",
        "    def create_model(self):\n",
        "        _model_ = self.define_model()\n",
        "        _model_.summary()\n",
        "        adam = Adam(lr=self.learning_rate, clipvalue=5.0)\n",
        "        _model_.compile(optimizer=adam, loss='sparse_categorical_crossentropy')\n",
        "        return _model_\n",
        "\n",
        "    # Function to append the START index to the response Y\n",
        "    def include_start_token(self, Y):\n",
        "        print(Y.shape)\n",
        "        Y = Y.reshape((Y.shape[0], Y.shape[1]))\n",
        "        Y = np.hstack((self.START * np.ones((Y.shape[0], 1)), Y[:, :-1]))\n",
        "        # Y = Y[:,:,np.newaxis]\n",
        "        return Y\n",
        "\n",
        "    def binarize_output_response(self, Y):\n",
        "        return np.array([np_utils.to_categorical(row, num_classes=self.max_vocab_size)\n",
        "                         for row in Y])\n",
        "\n",
        "    def respond_to_input(self, model, input_sent):\n",
        "        input_y = self.include_start_token(self.PAD * np.ones((1, self.max_seq_len)))\n",
        "        ids = np.array(self.words_to_indices(input_sent)).reshape((1, self.max_seq_len))\n",
        "        for pos in range(self.max_seq_len - 1):\n",
        "            pred = model.predict([ids, input_y]).argmax(axis=2)[0]\n",
        "            # pred = model.predict([ids, input_y])[0]\n",
        "            input_y[:, pos + 1] = pred[pos]\n",
        "        return self.indices_to_words(model.predict([ids, input_y]).argmax(axis=2)[0])\n",
        "\n",
        "    def train_model(self, model, X_train, X_test, y_train, y_test):\n",
        "        input_y_train = self.include_start_token(y_train)\n",
        "        print(input_y_train.shape)\n",
        "        print(input_y_train[0])\n",
        "        input_y_test = self.include_start_token(y_test)\n",
        "        print(input_y_test.shape)\n",
        "        print(input_y_test[0])\n",
        "        print(X_train.shape)\n",
        "        print(y_train.shape)\n",
        "        early = EarlyStopping(monitor='val_loss', patience=10, mode='auto')\n",
        "\n",
        "        checkpoint = ModelCheckpoint(self.outpath + 's2s_model_' + str(self.version) + '_.h5', monitor='val_loss',\n",
        "                                     verbose=1, save_best_only=True, mode='auto')\n",
        "        lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=0, mode='auto')\n",
        "        model.fit([X_train, input_y_train], y_train,\n",
        "                  epochs=self.epochs,\n",
        "                  batch_size=self.batch_size,\n",
        "                  validation_data=[[X_test, input_y_test], y_test],\n",
        "                  callbacks=[early, checkpoint, lr_reduce],\n",
        "                  shuffle=True)\n",
        "        return model\n",
        "\n",
        "    def generate_response(self, model, sentences):\n",
        "        output_responses = []\n",
        "        print(sentences)\n",
        "        for sent in sentences:\n",
        "            response = self.respond_to_input(model, sent)\n",
        "            output_responses.append(response)\n",
        "        out_df = pd.DataFrame()\n",
        "        out_df['Tweet in'] = sentences\n",
        "        out_df['Tweet out'] = output_responses\n",
        "        return out_df\n",
        "\n",
        "    def main(self):\n",
        "        if self.mode == 'train':\n",
        "            X_train, X_test, y_train, y_test, test_sentences = self.data_creation()\n",
        "            print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
        "            print('Data Creation completed')\n",
        "            model = self.create_model()\n",
        "            print(\"Model creation completed\")\n",
        "            print(\"mc \",X_train.shape,X_test.shape,y_train.shape,y_test.shape)\n",
        "            model = self.train_model(model, X_train, X_test, y_train, y_test)\n",
        "            test_responses = self.generate_response(model, test_sentences)\n",
        "            print(test_sentences)\n",
        "            print(test_responses)\n",
        "            pd.DataFrame(test_responses).to_csv(self.outpath + 'output_response.csv', index=False)\n",
        "            \n",
        "        elif self.mode == 'inference':\n",
        "            model = load_model(self.load_model_from)\n",
        "            self.vocabulary = joblib.load(self.vocabulary_path)\n",
        "            self.reverse_vocabulary = joblib.load(self.reverse_vocabulary_path)\n",
        "            # nalyzer_file = open(self.analyzer_path,\"rb\")\n",
        "            count_vectorizer = joblib.load(self.count_vectorizer_path)\n",
        "            self.analyzer = count_vectorizer.build_analyzer()\n",
        "            data = self.process_data(self.data_path)\n",
        "            col = data.columns.tolist()[0]\n",
        "            test_sentences = list(data[col].values)\n",
        "            test_sentences = self.replace_anonymized_names(test_sentences)\n",
        "            responses = self.generate_response(model, test_sentences)\n",
        "            print(responses)\n",
        "            responses.to_csv(self.outpath + 'responses_' + str(self.version) + '_.csv', index=False)\n",
        "\n",
        "\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owAFFIzjnZSB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b254ca43-5dc0-4942-a607-0620e6e66856"
      },
      "source": [
        "\n",
        "start_time = time()\n",
        "obj = chatbot()\n",
        "obj.mode = \"train\"\n",
        "obj.main()\n",
        "end_time = time()\n",
        "print(\"Processing finished, time taken is %s\", end_time - start_time)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "traintestsplit  (37500, 30) (12500, 30) (37500, 30, 1) (12500, 30, 1)\n",
            "(37500, 30) (37500, 30, 1) (12500, 30) (12500, 30, 1)\n",
            "Data Creation completed\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "last_word_input (InputLayer)    (None, 30)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder_input (InputLayer)      (None, 30)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 30, 100)      5000000     encoder_input[0][0]              \n",
            "                                                                 last_word_input[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "encoder (LSTM)                  (None, 100)          80400       embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_1 (RepeatVector)  (None, 30, 100)      0           encoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 30, 200)      0           embedding[1][0]                  \n",
            "                                                                 repeat_vector_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "decoder (LSTM)                  (None, 30, 100)      120400      concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "next_word_dense (TimeDistribute (None, 30, 2500)     252500      decoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "next_word_softmax (TimeDistribu (None, 30, 50000)    125050000   next_word_dense[0][0]            \n",
            "==================================================================================================\n",
            "Total params: 130,503,300\n",
            "Trainable params: 130,503,300\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model creation completed\n",
            "mc  (37500, 30) (12500, 30) (37500, 30, 1) (12500, 30, 1)\n",
            "(37500, 30, 1)\n",
            "(37500, 30)\n",
            "[2.0000e+00 6.1340e+03 4.1054e+04 4.5963e+04 1.6870e+04 9.8710e+03\n",
            " 4.1551e+04 4.1551e+04 3.0114e+04 7.6390e+03 4.1920e+03 1.0543e+04\n",
            " 4.5963e+04 3.1709e+04 1.5011e+04 4.5004e+04 4.1188e+04 4.4623e+04\n",
            " 2.5128e+04 6.1310e+03 6.2580e+03 3.9122e+04 1.0000e+00 1.0000e+00\n",
            " 1.0000e+00 1.0000e+00 1.0000e+00 1.0000e+00 1.0000e+00 1.0000e+00]\n",
            "(12500, 30, 1)\n",
            "(12500, 30)\n",
            "[2.0000e+00 6.1340e+03 1.8212e+04 2.2743e+04 4.1920e+03 2.0879e+04\n",
            " 3.4000e+04 1.5088e+04 2.3677e+04 2.1032e+04 4.4728e+04 4.5178e+04\n",
            " 6.5340e+03 4.1097e+04 3.6316e+04 3.0952e+04 4.1551e+04 3.0114e+04\n",
            " 1.8088e+04 3.8337e+04 1.1543e+04 4.1920e+03 4.4728e+04 7.3210e+03\n",
            " 4.1097e+04 3.9822e+04 4.1920e+03 4.1670e+03 6.9400e+03 1.0000e+00]\n",
            "(37500, 30)\n",
            "(37500, 30, 1)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 37500 samples, validate on 12500 samples\n",
            "Epoch 1/5\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "37500/37500 [==============================] - 190s 5ms/step - loss: 6.3555 - val_loss: 4.8241\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 4.82414, saving model to s2s_model_v1_.h5\n",
            "Epoch 2/5\n",
            "37500/37500 [==============================] - 177s 5ms/step - loss: 4.5773 - val_loss: 4.4481\n",
            "\n",
            "Epoch 00002: val_loss improved from 4.82414 to 4.44809, saving model to s2s_model_v1_.h5\n",
            "Epoch 3/5\n",
            "37500/37500 [==============================] - 177s 5ms/step - loss: 4.4069 - val_loss: 4.3631\n",
            "\n",
            "Epoch 00003: val_loss improved from 4.44809 to 4.36308, saving model to s2s_model_v1_.h5\n",
            "Epoch 4/5\n",
            "37500/37500 [==============================] - 177s 5ms/step - loss: 4.3032 - val_loss: 4.2432\n",
            "\n",
            "Epoch 00004: val_loss improved from 4.36308 to 4.24318, saving model to s2s_model_v1_.h5\n",
            "Epoch 5/5\n",
            "37500/37500 [==============================] - 177s 5ms/step - loss: 4.1775 - val_loss: 4.1145\n",
            "\n",
            "Epoch 00005: val_loss improved from 4.24318 to 4.11448, saving model to s2s_model_v1_.h5\n",
            "['@__cname__ hola tengo un paquete pendiente de entrega. No hemos coincidido. Como le hacemos para recibirlo?', \"I'm at McDonald's in Mesquite, TX https://t.co/jrUinetbPO\", 'Hi! Ordered One+3T on 21-feb-17 @__cname__ with CashBack Rs3000. Still waiting for cashBack @AmazonHelp @__cname__ ..order#406-6541696-8829138 https://t.co/5yT92zxIgx', '@__cname__ is all your advertising meant to be deceptive?  Or, is it just your \"buy service\"? #deceptive #advertising https://t.co/XUhmuxiNpJ', '@__cname__ Also, can you tell me where the CD I just imported went? Cause iTunes sure canâ€™t.', 'Why does @__cname__  even bother offering two day shipping when they send it regular mail via @__cname__ .  It never arrives in that time frame.', 'Running around Prague lost out of our minds no one knows the Airbnb address', '@sprintcare The supervisor that talked to me yesterday was rude and didnâ€™t care about my issues! Please donâ€™t say I canâ€™t upgrade a replacement phone anywhere but Sprint! Then say for me to go to AT&amp;T..', 'My phone is showing me empty lock screen all the time and when I force reboot Iâ€™m getting last weeks message notifications and couple hours later same bs all over again! @AppleSupport tell me what to do I donâ€™t wanna wait for next update this bug is so irritating ðŸ˜ ', 'Members of staff smoking at Penicuik @Tesco next to fire exit and taxi pick up point - not good']\n",
            "(1, 30)\n",
            "(1, 30)\n",
            "(1, 30)\n",
            "(1, 30)\n",
            "(1, 30)\n",
            "(1, 30)\n",
            "(1, 30)\n",
            "(1, 30)\n",
            "(1, 30)\n",
            "(1, 30)\n",
            "['@__cname__ hola tengo un paquete pendiente de entrega. No hemos coincidido. Como le hacemos para recibirlo?', \"I'm at McDonald's in Mesquite, TX https://t.co/jrUinetbPO\", 'Hi! Ordered One+3T on 21-feb-17 @__cname__ with CashBack Rs3000. Still waiting for cashBack @AmazonHelp @__cname__ ..order#406-6541696-8829138 https://t.co/5yT92zxIgx', '@__cname__ is all your advertising meant to be deceptive?  Or, is it just your \"buy service\"? #deceptive #advertising https://t.co/XUhmuxiNpJ', '@__cname__ Also, can you tell me where the CD I just imported went? Cause iTunes sure canâ€™t.', 'Why does @__cname__  even bother offering two day shipping when they send it regular mail via @__cname__ .  It never arrives in that time frame.', 'Running around Prague lost out of our minds no one knows the Airbnb address', '@sprintcare The supervisor that talked to me yesterday was rude and didnâ€™t care about my issues! Please donâ€™t say I canâ€™t upgrade a replacement phone anywhere but Sprint! Then say for me to go to AT&amp;T..', 'My phone is showing me empty lock screen all the time and when I force reboot Iâ€™m getting last weeks message notifications and couple hours later same bs all over again! @AppleSupport tell me what to do I donâ€™t wanna wait for next update this bug is so irritating ðŸ˜ ', 'Members of staff smoking at Penicuik @Tesco next to fire exit and taxi pick up point - not good']\n",
            "                                            Tweet in                          Tweet out\n",
            "0  @__cname__ hola tengo un paquete pendiente de ...  @__cname__ @__cname__ , , , , . .\n",
            "1  I'm at McDonald's in Mesquite, TX https://t.co...          @__cname__ hi , , , , . .\n",
            "2  Hi! Ordered One+3T on 21-feb-17 @__cname__ wit...          @__cname__ hi , , , , . .\n",
            "3  @__cname__ is all your advertising meant to be...          @__cname__ hi , , , , . .\n",
            "4  @__cname__ Also, can you tell me where the CD ...          @__cname__ hi , , , , . .\n",
            "5  Why does @__cname__  even bother offering two ...          @__cname__ hi , , , , . .\n",
            "6  Running around Prague lost out of our minds no...          @__cname__ hi , , , , . .\n",
            "7  @sprintcare The supervisor that talked to me y...          @__cname__ hi , , , , . .\n",
            "8  My phone is showing me empty lock screen all t...          @__cname__ hi , , , , . .\n",
            "9  Members of staff smoking at Penicuik @Tesco ne...          @__cname__ hi , , , , . .\n",
            "Processing finished, time taken is %s 968.9307947158813\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFumA0VuDvE5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}